{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from ast import literal_eval\n",
    "import re, string\n",
    "\n",
    "\n",
    "txt = open('dataset.txt')\n",
    "raw = txt.read()\n",
    "\n",
    "def preprocess_word(w):\n",
    "    # Removes punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    punctuation = w.translate(translator)\n",
    "\n",
    "    return punctuation\n",
    "\n",
    "\n",
    "def preprocessing(x):\n",
    "    # Returns a nested list of the processed sentences\n",
    "    \n",
    "    mentions = re.sub(r'@\\w+',\"\", x) #removes mentions\n",
    "    numbers = re.sub('[0-9]+', \"\", mentions) #removes numbers\n",
    "    links = re.sub(r'http\\S+', \"\", numbers) #removes links\n",
    "    \n",
    "    sentence_tokenized = sent_tokenize(links) #sentence tokenization - INACCURATE!!!\n",
    "    \n",
    "    lower = [[sent.lower()] for sent in sentence_tokenized] #lower text\n",
    "     \n",
    "    in_list = [word for sent in lower for word in sent]\n",
    "\n",
    "    word_tokenized = [word_tokenize(sent) for sent in in_list]\n",
    "    word_tokenized = [sent for sent in word_tokenized if sent] #word tokenization\n",
    "    \n",
    "    for _id, sent in enumerate(word_tokenized):\n",
    "        word_tokenized[_id] =  [preprocess_word(w) for w in sent]\n",
    "    \n",
    "    words = [[word for word in sent if word != '' and word != 'rt'] for sent in word_tokenized] #removes useless words\n",
    "    sentences = [sent for sent in words if sent] #removes empty sentences\n",
    "    \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = preprocessing(raw)\n",
    "\n",
    "df = pd.DataFrame(text)\n",
    "writer = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='welcome', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG OF WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from ast import literal_eval\n",
    "\n",
    "try:\n",
    "    assert(literal_eval(str(text)) == text.copy())\n",
    "except AssertionError:\n",
    "    print('failed to convert')\n",
    "    \n",
    "final_str = [\" \".join(x) for x in text]\n",
    "\n",
    "train, test = train_test_split(final_str, test_size=0.2)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train)\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "vector = model.wv['bitch'] #returns numpy vector of a word\n",
    "\n",
    "sims = model.wv.most_similar('bitch', topn=10) #returns similar words\n",
    "\n",
    "print(vector)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA  \n",
    "from sklearn.manifold import TSNE                 \n",
    "import numpy as np                                  \n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2 \n",
    "\n",
    "\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key) \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(autosize=True)\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SYNTAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(x):\n",
    "#Takes a nested list and converts it into a list of elements\n",
    "#where every sublist is a new element\n",
    "\n",
    "    new_list = [] \n",
    "    \n",
    "    for sent in text:\n",
    "        sentences = \" \".join(sent)\n",
    "        new_list.append(sentences)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "new = flatten_list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing(x):\n",
    "# Returns a nested list of syntactic labels\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    dependencies = []\n",
    "    for sent in x:\n",
    "        doc = nlp(sent)\n",
    "        new_list = [token.dep_ for token in doc]\n",
    "        dependencies.append(new_list)\n",
    "        \n",
    "    return dependencies\n",
    "\n",
    "dep = dependency_parsing(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "wordset = set()\n",
    "for sentence in text:\n",
    "    for word in sentence:\n",
    "        wordset.add(word)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Add every word as a node\n",
    "base_graph = nx.Graph()\n",
    "base_graph.add_nodes_from(wordset)\n",
    "nx.draw(base_graph, with_labels=True)\n",
    "\n",
    "graphs = {}\n",
    "for sentence_id, sentence_contents in enumerate(new):\n",
    "    sentence_graph = base_graph.copy()\n",
    "    processed_sentence =  nlp(' '.join(new))\n",
    "print('\\n',processed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Add edges between the nodes according to syntactic relations\n",
    "for token in processed_sentence:\n",
    "    nodeA = token.text\n",
    "    nodeB = token.head.text\n",
    "    print('\\tadding edge between', nodeA, 'and', nodeB)\n",
    "    sentence_graph.add_edge(nodeA, nodeB)\n",
    "    sentence_representation =  nx.adjacency_matrix(sentence_graph) #sparse matrix\n",
    "    print('\\t sparse matrix has ',sentence_representation.count_nonzero(),'nonzero elements')\n",
    "    graphs[sentence_id] = sentence_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(sentence_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nsaving representations...')\n",
    "import pickle\n",
    "with open('sentence_representations_adjmat.p','wb') as fw:\n",
    "    pickle.dump(graphs, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "train, test = train_test_split(train_counts, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "pred = gnb.fit(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
