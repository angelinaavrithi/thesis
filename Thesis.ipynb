{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from ast import literal_eval\n",
    "import re, string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df =  pd.read_csv(\"dataset.txt\")\n",
    "tweets = df['tweet'].tolist()\n",
    "\n",
    "def preprocess_word(w):\n",
    "    # Removes punctuation\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    punctuation = w.translate(translator)\n",
    "\n",
    "    return punctuation\n",
    "\n",
    "\n",
    "def preprocessing(x):\n",
    "    # Returns a nested list of the processed sentences\n",
    "    \n",
    "    mentions = [re.sub(r'@\\w+',\"\", sent) for sent in tweets] #removes mentions\n",
    "    numbers = [re.sub('[0-9]+', \"\", sent) for sent in mentions] #removes numbers\n",
    "    links = [re.sub(r'http\\S+', \"\", sent) for sent in numbers] #removes links\n",
    "    \n",
    "    lower = [[sent.lower()] for sent in links] #lower text\n",
    "    in_list = [word for sent in lower for word in sent]\n",
    "    word_tokenized = [word_tokenize(sent) for sent in in_list]\n",
    "    word_tokenized = [sent for sent in word_tokenized if sent] #word tokenization\n",
    "    \n",
    "    for _id, sent in enumerate(word_tokenized):\n",
    "        word_tokenized[_id] =  [preprocess_word(w) for w in sent]\n",
    "\n",
    "    words = [[word for word in sent if word != '' and word != 'rt' and len(word)>1] for sent in word_tokenized] #removes useless words\n",
    "    sentences = [sent for sent in words if sent] #removes empty sentences\n",
    "\n",
    "    return sentences\n",
    "\n",
    "text = preprocessing(tweets)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG OF WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from ast import literal_eval\n",
    "\n",
    "try: \n",
    "    assert(literal_eval(str(text)) == text.copy())\n",
    "except AssertionError:\n",
    "    print('failed to convert')\n",
    "    \n",
    "final_str = ([\" \".join(x) for x in text])\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(final_str).toarray()\n",
    "print(len(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n"
     ]
    }
   ],
   "source": [
    "vocab = count_vect.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n"
     ]
    }
   ],
   "source": [
    "#Returns the frequency of every word in total\n",
    "sumindex = [sum(x) for x in zip(*bow)]\n",
    "print(len(sumindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf = count_vect.fit_transform(final_str).toarray()\n",
    "print(tfidf)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angelina\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "model = Word2Vec(sentences=text, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "vector = model.wv['no'] #returns numpy vector of a word\n",
    "sims = model.wv.most_similar('no', topn=10) #returns similar words\n",
    "\n",
    "embeddings = [model.wv[word] for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the word vector average for every sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_average = []\n",
    "for i in text:\n",
    "    av = np.mean(model.wv[i], axis=0)\n",
    "    v_average.append(av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA  \n",
    "from sklearn.manifold import TSNE                 \n",
    "import numpy as np                                  \n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2 \n",
    "\n",
    "\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key) \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(autosize=True)\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SYNTAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(x):\n",
    "#Takes a nested list and converts it into a list of elements\n",
    "#where every sublist is a new element\n",
    "\n",
    "    new_list = [] \n",
    "    \n",
    "    for sent in x:\n",
    "        sentences = \" \".join(sent)\n",
    "        new_list.append(sentences)\n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def dependency_parsing(x):\n",
    "# Returns a nested list of syntactic labels\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    dependencies = []\n",
    "    for sent in x:\n",
    "        doc = nlp(sent)\n",
    "        new_list = [token.dep_ for token in doc]\n",
    "        dependencies.append(new_list)\n",
    "        \n",
    "    return dependencies\n",
    "\n",
    "dep = dependency_parsing(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "wordset = set()\n",
    "for sentence in text:\n",
    "    for word in sentence:\n",
    "        wordset.add(word)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Add every word as a node\n",
    "base_graph = nx.Graph()\n",
    "base_graph.add_nodes_from(wordset)\n",
    "nx.draw(base_graph, with_labels=True)\n",
    "\n",
    "graphs = {}\n",
    "for sentence_id, sentence_contents in enumerate(new):\n",
    "    sentence_graph = base_graph.copy()\n",
    "    processed_sentence =  nlp(' '.join(new))\n",
    "print('\\n',processed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Add edges between the nodes according to syntactic relations\n",
    "for token in processed_sentence:\n",
    "    nodeA = token.text\n",
    "    nodeB = token.head.text\n",
    "    #print('\\tadding edge between', nodeA, 'and', nodeB)\n",
    "    sentence_graph.add_edge(nodeA, nodeB)\n",
    "    sentence_representation =  nx.adjacency_matrix(sentence_graph) #sparse matrix\n",
    "    #print('\\t sparse matrix has ',sentence_representation.count_nonzero(),'nonzero elements')\n",
    "    graphs[sentence_id] = sentence_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"font_size\": 20,\n",
    "    \"node_size\": 30,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": 'blue',\n",
    "    \"linewidths\": 2,\n",
    "    \"width\": 2,\n",
    "}\n",
    "plt.figure(3,figsize=(33,33)) \n",
    "nx.draw(sentence_graph, with_labels=True, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sentence_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nsaving representations...')\n",
    "import pickle\n",
    "with open('sentence_representations_adjmat.p','wb') as fw:\n",
    "    pickle.dump(graphs, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(columns=['Tweets','Class','BoW','Embeddings'])\n",
    "A['Tweets'] = [sent for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['Class'] = df['class']\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency & Embeddings lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrequency_list = []\n",
    "for sent in text:\n",
    "    for word in sent:\n",
    "        ind = vocab.index(word)\n",
    "        #print('\\n',word, '\\nIndex:', ind)\n",
    "        freq = sumindex[ind]\n",
    "        #print(\"BOW frequency:\", freq)\n",
    "        wfrequency_list.append(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wembeddings_list = []\n",
    "for l in embeddings:\n",
    "    for subl in l:\n",
    "        wembeddings_list.append(subl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification using Bag-of-Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bow\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "logr = LogisticRegression()\n",
    "logr.fit(x_train, y_train)\n",
    "bow_predictions = logr.predict(x_test)\n",
    "print(bow_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.82      0.98      0.89        55\n",
      "           2       0.83      0.33      0.48        15\n",
      "\n",
      "    accuracy                           0.82        72\n",
      "   macro avg       0.55      0.44      0.46        72\n",
      "weighted avg       0.80      0.82      0.78        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_report = classification_report(y_test, bow_predictions)\n",
    "print(bow_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification using embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "v_train, v_test, y_train, y_test = train_test_split(v_average, y, test_size=0.25, random_state=0)\n",
    "logr.fit(v_train, y_train)\n",
    "emb_predictions = logr.predict(v_test)\n",
    "print(emb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.76      1.00      0.87        55\n",
      "           2       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.76        72\n",
      "   macro avg       0.25      0.33      0.29        72\n",
      "weighted avg       0.58      0.76      0.66        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_report = classification_report(y_test, emb_predictions)\n",
    "print(emb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification using both Bag-of-Words and embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = np.concatenate([bow, v_average], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "c_train, c_test, y_train, y_test = train_test_split(conc, y, test_size=0.25, random_state=0)\n",
    "logr.fit(c_train, y_train)\n",
    "bow_emb_predictions = logr.predict(c_test)\n",
    "print(bow_emb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.82      0.98      0.89        55\n",
      "           2       0.83      0.33      0.48        15\n",
      "\n",
      "    accuracy                           0.82        72\n",
      "   macro avg       0.55      0.44      0.46        72\n",
      "weighted avg       0.80      0.82      0.78        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angelina\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "bow_emb_report = classification_report(y_test, bow_emb_predictions)\n",
    "print(bow_emb_report)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
